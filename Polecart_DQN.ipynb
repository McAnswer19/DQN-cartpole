{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from collections import deque\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import os\n",
    "from matplotlib.ticker import FormatStrFormatter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "state_size = env.observation_space.shape[0]  # For cartpole, states are: position, velocity, angle, angular velocity.\n",
    "action_size = env.action_space.n          # For cartpole, actions are: [-1, 1 ], forwards and back\n",
    "\n",
    "print('state_vector_size:', state_size)\n",
    "print('action_vector_size:', action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a method to create a Deep-Q network for the agent. \n",
    "def q_network(numInput, numHidden, numHiddenLayers, numOutput, optimiserFunction=tf.train.AdamOptimizer,\n",
    "              alpha=0.00025, lossFunction=\"mse\", hiddenActivation=\"relu\", outputActivation=\"linear\"): \n",
    "    \n",
    "    # 0.00005\n",
    "    \n",
    "    #Creating a TensorFlow class\n",
    "    network = Sequential()\n",
    "    \n",
    "    #Creating the input layer\n",
    "    network.add(Dense(numHidden, input_dim=numInput, activation=hiddenActivation))\n",
    "    \n",
    "    #Adding 'n' hidden layers\n",
    "    for _ in range(numHiddenLayers):\n",
    "        network.add(Dense(numHidden, activation=hiddenActivation))\n",
    "        \n",
    "    #Creating output layer\n",
    "    network.add(Dense(numOutput, activation=outputActivation))\n",
    "    \n",
    "    #Defining the loss function, the optimiser and the metrics.\n",
    "    network.compile(loss=lossFunction, optimizer=Adam(lr=alpha))\n",
    "    \n",
    "    return(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full agent class. Implements dumb/random experience replay. Also take note of how large tau is. \n",
    "class DQN_Agent():\n",
    "    def __init__(self, environment, numObservations, numActions, numHidden, numHiddenLayer, modelFileName,\n",
    "                     gamma=0.99, alpha=0.00005, epsilon=1, epsilonDecay=0.995, epsilon_min=0, tau=0.5,\n",
    "                     load=False, save=True):\n",
    "\n",
    "        self.numObservations = numObservations\n",
    "        self.numActions = numActions\n",
    "        self.numHidden = numHidden\n",
    "        self.numHiddenLayer = numHiddenLayer\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.epsilonDecay = epsilonDecay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        #replay buffer\n",
    "        self.replay_buffer = []\n",
    "        \n",
    "        # The networks\n",
    "        self.q_network = q_network(self.numObservations,self.numHidden,self.numHiddenLayer,self.numActions)\n",
    "        self.q_target_network = q_network(self.numObservations,self.numHidden,self.numHiddenLayer,self.numActions)\n",
    "                \n",
    "        if(load):\n",
    "            self.q_network.load_weights(modelFileName)\n",
    "      \n",
    "    \n",
    "        self.soft_target_weight_update(1)\n",
    "    \n",
    "    def updateEpsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilonDecay\n",
    "        \n",
    "        if(self.epsilon < self.epsilon_min):\n",
    "            self.epsilon = self.epsilon_min\n",
    "    \n",
    "    def select_epsilon_greedy_action(self, state):            \n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return(env.action_space.sample())  # random action\n",
    "        else:\n",
    "            q_values = self.q_network.predict(state)\n",
    "            return(np.argmax(q_values[0]))        # greedy action\n",
    "\n",
    "    def soft_target_weight_update(self, tau):\n",
    "        # Q network weights\n",
    "        weights = np.asarray(self.q_network.get_weights())\n",
    "        \n",
    "        # Target network weights\n",
    "        target_weights = np.asarray(self.q_target_network.get_weights())\n",
    "        \n",
    " \n",
    "        # We set the weights according to XXX. \n",
    "        self.q_target_network.set_weights( (weights * tau) + (target_weights * (1 - tau)))\n",
    "\n",
    "    def learn_from_transition(self, action, state,next_state, reward, done):\n",
    "        q_values = self.q_network.predict(state)\n",
    "\n",
    "        target_q_values = q_values\n",
    "        \n",
    "        if done == True: \n",
    "            target_q_values[0][action] = reward\n",
    "        else: \n",
    "            target_q_values[0][action] = reward + (self.gamma * np.max(self.q_target_network.predict(next_state)) )\n",
    "        \n",
    "        self.q_network.fit(state, target_q_values, epochs=4, batch_size=32, verbose=0)\n",
    "\n",
    "    def learn_from_m_random_transitions_in_replay_buffer(self, number_of_trans = 1):\n",
    "        \n",
    "        assert number_of_trans >= 1\n",
    "        \n",
    "        size_of_replay_buffer = len(self.replay_buffer)\n",
    "        for i in range(number_of_trans):                \n",
    "            transition_index = random.randint(0, size_of_replay_buffer-1)\n",
    "            action, state, next_state, reward, done_bool = self.replay_buffer[transition_index]\n",
    "            self.learn_from_transition(action, state, next_state, reward, done_bool)\n",
    "            \n",
    "            \n",
    "        self.soft_target_weight_update(self.tau)  # Always do a target weight update afterwards. \n",
    "        \n",
    "        \n",
    "    def add_to_replay_buffer(self, action, current_state, next_state, reward, done): \n",
    "        transition_tuple = (action, current_state, next_state, reward, done)\n",
    "        self.replay_buffer.append(transition_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training. This cell contains the agent/environment loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the state/action space, respectively\n",
    "numObservation = env.observation_space.shape[0]\n",
    "numActions = env.action_space.n\n",
    "\n",
    "\n",
    "numHidden = 24                  # number of neurons. \n",
    "numHiddenLayer = 2              # number of hidden layer.\n",
    "\n",
    "\n",
    "modelFileName = \"Polecart_weights.h5\"\n",
    "\n",
    "agent = DQN_Agent(env, numObservation, numActions, numHidden, numHiddenLayer, modelFileName,\n",
    "                    save=True, load=False)\n",
    "\n",
    "\n",
    "starting_state = env.reset() # Getting things ready\n",
    "\n",
    "num_episodes = 2000 \n",
    "max_steps = 10**6     # Effectively infinite for our purposes, as it should be. \n",
    "\n",
    "reward_array = np.ndarray(shape = (num_episodes, max_steps))\n",
    "steps_array = np.ndarray(shape = (num_episodes))\n",
    "\n",
    "\n",
    "rewardList = []\n",
    "meanRewards = []\n",
    "episodeList = []\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    step = 1\n",
    "    reward_sum = 0\n",
    "    \n",
    "    # reseting the environment and getting the starting episode action. \n",
    "    current_state = env.reset().reshape(1, numObservation)\n",
    "    chosen_action = agent.select_epsilon_greedy_action(current_state)    \n",
    "    \n",
    "# The main agent/environment loop for a given episode ------------------------------\n",
    "    done = False\n",
    "    while done == False and step < max_steps:   # The problem is here:\n",
    "            \n",
    "        next_state, reward, done, info = env.step(chosen_action)\n",
    "        next_state = next_state.reshape(1, numObservation)  # Tedious reshaping needed.\n",
    "        \n",
    "        agent.add_to_replay_buffer(chosen_action, current_state, next_state, reward, done)\n",
    "        \n",
    "        # Setting things up for the next iteration of the while loop \n",
    "        current_state = next_state\n",
    "        chosen_action = agent.select_epsilon_greedy_action(current_state)\n",
    "        \n",
    "        step += 1\n",
    "        reward_sum += reward\n",
    "        \n",
    "        # If we are on the last episode, render the agent playing\n",
    "        if(episode == num_episodes-1):\n",
    "            env.render()\n",
    "            \n",
    "    steps_array[episode] = step    \n",
    "    \n",
    "    # We randomly sample 50 state-action transitions after every episode. \n",
    "    agent.learn_from_m_random_transitions_in_replay_buffer(50)\n",
    "    rewardList.append(reward_sum)\n",
    "    episodeList.append(agent.epsilon)\n",
    "    \n",
    "    agent.updateEpsilon()\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Down here is just plotting code. it can be safely removed without affecting anything else. \n",
    "    if(episode > 20):\n",
    "        meanRewards.append(np.mean(rewardList[-19:]))\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    ax = plt.subplot(2,2,1)\n",
    "    #ax.plot(range(episode+1), rewardList)\n",
    "    ax2 = ax.twinx()\n",
    "    ax.plot(range(episode+1), rewardList, 'g-')\n",
    "    ax2.plot(range(episode+1), episodeList, 'b-')\n",
    "\n",
    "    ax = plt.subplot(2,2,2)\n",
    "    ax.plot(range(len(meanRewards)), meanRewards)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "#Saving the trained model\n",
    "agent.q_network.save_weights(modelFileName)\n",
    "env.close()\n",
    "\n",
    "# More plotting. \n",
    "plt.plot(range(num_episodes), rewardList)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "408px",
    "left": "284.364px",
    "right": "20px",
    "top": "145px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
